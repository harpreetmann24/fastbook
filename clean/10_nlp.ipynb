{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"K_N9XeHzzeNe","executionInfo":{"status":"ok","timestamp":1651277662669,"user_tz":300,"elapsed":6694,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["#hide\n","! [ -e /content ] && pip install -Uqq fastbook\n","import fastbook\n","fastbook.setup_book()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PzUZwCvFzeNh","executionInfo":{"status":"ok","timestamp":1651277662670,"user_tz":300,"elapsed":5,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["#hide\n","from fastbook import *\n","from IPython.display import display,HTML"]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BnNC6RGzyB_","executionInfo":{"status":"ok","timestamp":1651277662795,"user_tz":300,"elapsed":129,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"e8442fb6-46f5-4b2c-bea1-386b84d70632"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"markdown","metadata":{"id":"e5dq0Q0tzeNh"},"source":["# NLP Deep Dive: RNNs"]},{"cell_type":"markdown","metadata":{"id":"yFskbWAqzeNi"},"source":["## Text Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"vYOpdXq4zeNi"},"source":["### Tokenization"]},{"cell_type":"markdown","metadata":{"id":"OIhiv_q1zeNj"},"source":["### Word Tokenization with fastai"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"c6vndUMyzeNj","executionInfo":{"status":"ok","timestamp":1651277664457,"user_tz":300,"elapsed":246,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}}},"outputs":[],"source":["from fastai.text.all import *"]},{"cell_type":"code","source":["\n","path = untar_data(URLs.IMDB)"],"metadata":{"id":"2logAzK_z0Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nf9qIm60zeNj"},"outputs":[],"source":["files = get_text_files(path, folders = ['train', 'test', 'unsup'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OXBktkekzeNk"},"outputs":[],"source":["txt = files[0].open().read(); txt[:75]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXkLn1kuzeNk"},"outputs":[],"source":["spacy = WordTokenizer()\n","toks = first(spacy([txt]))\n","print(coll_repr(toks, 30))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfOoGW71zeNl"},"outputs":[],"source":["first(spacy(['The U.S. dollar $1 is $1.00.']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1W-0ylozeNl"},"outputs":[],"source":["tkn = Tokenizer(spacy)\n","print(coll_repr(tkn(txt), 31))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKr1DfZyzeNl"},"outputs":[],"source":["defaults.text_proc_rules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-LBSqMwzeNm"},"outputs":[],"source":["coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"]},{"cell_type":"markdown","metadata":{"id":"Bzv3hef7zeNm"},"source":["### Subword Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMEezdXUzeNm"},"outputs":[],"source":["txts = L(o.open().read() for o in files[:2000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goxAQWqqzeNm"},"outputs":[],"source":["def subword(sz):\n","    sp = SubwordTokenizer(vocab_sz=sz)\n","    sp.setup(txts)\n","    return ' '.join(first(sp([txt]))[:40])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sa1LhlklzeNm"},"outputs":[],"source":["subword(1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDnGJ1iFzeNn"},"outputs":[],"source":["subword(200)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drh-9Ss4zeNn"},"outputs":[],"source":["subword(10000)"]},{"cell_type":"markdown","metadata":{"id":"ybqw-DpIzeNn"},"source":["### Numericalization with fastai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJlM95KWzeNn"},"outputs":[],"source":["toks = tkn(txt)\n","print(coll_repr(tkn(txt), 31))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adc0ZfVdzeNn"},"outputs":[],"source":["toks200 = txts[:200].map(tkn)\n","toks200[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2k9f__OzeNn"},"outputs":[],"source":["num = Numericalize()\n","num.setup(toks200)\n","coll_repr(num.vocab,20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSuhQwebzeNo"},"outputs":[],"source":["nums = num(toks)[:20]; nums"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBcy-11BzeNo"},"outputs":[],"source":["' '.join(num.vocab[o] for o in nums)"]},{"cell_type":"markdown","metadata":{"id":"Ua_VXmFPzeNo"},"source":["### Putting Our Texts into Batches for a Language Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gO3w7G9bzeNo"},"outputs":[],"source":["stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n","tokens = tkn(stream)\n","bs,seq_len = 6,15\n","d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n","df = pd.DataFrame(d_tokens)\n","display(HTML(df.to_html(index=False,header=None)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZdMBhJMzeNo"},"outputs":[],"source":["bs,seq_len = 6,5\n","d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n","df = pd.DataFrame(d_tokens)\n","display(HTML(df.to_html(index=False,header=None)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1vh2Sb1zeNo"},"outputs":[],"source":["bs,seq_len = 6,5\n","d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n","df = pd.DataFrame(d_tokens)\n","display(HTML(df.to_html(index=False,header=None)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"seAb1GtwzeNp"},"outputs":[],"source":["bs,seq_len = 6,5\n","d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n","df = pd.DataFrame(d_tokens)\n","display(HTML(df.to_html(index=False,header=None)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swFq0vbIzeNp"},"outputs":[],"source":["nums200 = toks200.map(num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nRdvbZ-zeNp"},"outputs":[],"source":["dl = LMDataLoader(nums200)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAPkPwYizeNp"},"outputs":[],"source":["x,y = first(dl)\n","x.shape,y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNCV5ItRzeNp"},"outputs":[],"source":["' '.join(num.vocab[o] for o in x[0][:20])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDPGfqSDzeNp"},"outputs":[],"source":["' '.join(num.vocab[o] for o in y[0][:20])"]},{"cell_type":"markdown","metadata":{"id":"ljOtAOMGzeNp"},"source":["## Training a Text Classifier"]},{"cell_type":"markdown","metadata":{"id":"VTtiE2C_zeNp"},"source":["### Language Model Using DataBlock"]},{"cell_type":"code","source":["!pip install nbdev"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sAtrWSLR0MW5","executionInfo":{"status":"ok","timestamp":1651277629525,"user_tz":300,"elapsed":6277,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"fe4b8a1d-b94a-4ebe-e074-7a10fd6a0c21"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nbdev\n","  Downloading nbdev-1.2.6-py3-none-any.whl (50 kB)\n","\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 30 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40 kB 39.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n","\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from nbdev) (4.10.1)\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from nbdev) (21.1.3)\n","Requirement already satisfied: Jinja2<3.1.0 in /usr/local/lib/python3.7/dist-packages (from nbdev) (2.11.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from nbdev) (3.13)\n","Requirement already satisfied: jupyter-client<8 in /usr/local/lib/python3.7/dist-packages (from nbdev) (5.3.5)\n","Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from nbdev) (5.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nbdev) (21.3)\n","Collecting nbconvert>=6.1\n","  Downloading nbconvert-6.5.0-py3-none-any.whl (561 kB)\n","\u001b[K     |████████████████████████████████| 561 kB 60.7 MB/s \n","\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from nbdev) (1.0.0)\n","Requirement already satisfied: fastcore>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbdev) (1.4.2)\n","Collecting ghapi\n","  Downloading ghapi-0.1.20-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n","\u001b[?25hCollecting fastrelease\n","  Downloading fastrelease-0.1.16-py3-none-any.whl (13 kB)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.1.0->nbdev) (2.0.1)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (4.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (2.8.2)\n","Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (5.1.1)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (5.1.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (22.3.0)\n","Collecting Jinja2<3.1.0\n","  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 63.7 MB/s \n","\u001b[?25hRequirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.6.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (5.0.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (4.6.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (1.5.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (1.1.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.2.2)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.8.4)\n","Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (2.6.1)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.5.0->nbconvert>=6.1->nbdev) (1.5.5)\n","Collecting jupyter-client<8\n","  Downloading jupyter_client-7.3.0-py3-none-any.whl (130 kB)\n","\u001b[K     |████████████████████████████████| 130 kB 46.2 MB/s \n","\u001b[?25hCollecting tornado>=4.1\n","  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n","\u001b[K     |████████████████████████████████| 428 kB 49.9 MB/s \n","\u001b[?25hRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4.0->nbdev) (2.15.3)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4.0->nbdev) (4.3.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4.0->nbdev) (0.18.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4.0->nbdev) (21.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4.0->nbdev) (4.2.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4.0->nbdev) (5.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4.0->nbdev) (4.11.3)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.4.0->nbdev) (3.8.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client<8->nbdev) (1.15.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=6.1->nbdev) (0.5.1)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->nbdev) (5.5.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (57.4.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (0.8.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (0.7.5)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (4.4.2)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (1.0.18)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->nbdev) (0.2.5)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (5.3.0)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (5.2.0)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (7.7.0)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (5.3.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nbdev) (1.1.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nbdev) (3.6.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nbdev) (0.2.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nbdev) (0.13.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nbdev) (1.8.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->nbdev) (0.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->nbdev) (3.0.8)\n","Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nbdev) (2.0.1)\n","Installing collected packages: tornado, jupyter-client, Jinja2, nbconvert, ghapi, fastrelease, nbdev\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 5.1.1\n","    Uninstalling tornado-5.1.1:\n","      Successfully uninstalled tornado-5.1.1\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter-client 5.3.5\n","    Uninstalling jupyter-client-5.3.5:\n","      Successfully uninstalled jupyter-client-5.3.5\n","  Attempting uninstall: Jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Uninstalling Jinja2-2.11.3:\n","      Successfully uninstalled Jinja2-2.11.3\n","  Attempting uninstall: nbconvert\n","    Found existing installation: nbconvert 5.6.1\n","    Uninstalling nbconvert-5.6.1:\n","      Successfully uninstalled nbconvert-5.6.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\n","flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.0.3 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed Jinja2-3.0.3 fastrelease-0.1.16 ghapi-0.1.20 jupyter-client-7.3.0 nbconvert-6.5.0 nbdev-1.2.6 tornado-6.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["jupyter_client","tornado"]}}},"metadata":{}}]},{"cell_type":"code","source":["doc(DataBlock().dataloaders)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"O-xLB0EC0B-d","executionInfo":{"status":"ok","timestamp":1651277697442,"user_tz":300,"elapsed":118,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"}},"outputId":"4349c82c-8853-421b-afd2-8e45a66f0678"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<h4 id=\"DataBlock.dataloaders\" class=\"doc_header\"><code>DataBlock.dataloaders</code><a href=\"https://github.com/fastai/fastai/tree/master/fastai/data/block.py#L112\" class=\"source_link\" style=\"float:right\">[source]</a></h4><blockquote><p><code>DataBlock.dataloaders</code>(<strong><code>source</code></strong>, <strong><code>path</code></strong>=<em><code>'.'</code></em>, <strong><code>verbose</code></strong>=<em><code>False</code></em>, <strong><code>bs</code></strong>=<em><code>64</code></em>, <strong><code>shuffle</code></strong>=<em><code>False</code></em>, <strong><code>num_workers</code></strong>=<em><code>None</code></em>, <strong><code>do_setup</code></strong>=<em><code>True</code></em>, <strong><code>pin_memory</code></strong>=<em><code>False</code></em>, <strong><code>timeout</code></strong>=<em><code>0</code></em>, <strong><code>batch_size</code></strong>=<em><code>None</code></em>, <strong><code>drop_last</code></strong>=<em><code>False</code></em>, <strong><code>indexed</code></strong>=<em><code>None</code></em>, <strong><code>n</code></strong>=<em><code>None</code></em>, <strong><code>device</code></strong>=<em><code>None</code></em>, <strong><code>persistent_workers</code></strong>=<em><code>False</code></em>, <strong><code>wif</code></strong>=<em><code>None</code></em>, <strong><code>before_iter</code></strong>=<em><code>None</code></em>, <strong><code>after_item</code></strong>=<em><code>None</code></em>, <strong><code>before_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_batch</code></strong>=<em><code>None</code></em>, <strong><code>after_iter</code></strong>=<em><code>None</code></em>, <strong><code>create_batches</code></strong>=<em><code>None</code></em>, <strong><code>create_item</code></strong>=<em><code>None</code></em>, <strong><code>create_batch</code></strong>=<em><code>None</code></em>, <strong><code>retain</code></strong>=<em><code>None</code></em>, <strong><code>get_idxs</code></strong>=<em><code>None</code></em>, <strong><code>sample</code></strong>=<em><code>None</code></em>, <strong><code>shuffle_fn</code></strong>=<em><code>None</code></em>, <strong><code>do_batch</code></strong>=<em><code>None</code></em>)</p>\n","</blockquote>\n","<p>Create a <code>DataLoaders</code> object from <code>source</code></p>\n","<p><a href=\"https://docs.fast.ai/data.block#DataBlock.dataloaders\" target=\"_blank\" rel=\"noreferrer noopener\">Show in docs</a></p>\n","<style>\n","    table { border-collapse: collapse; border:thin solid #dddddd; margin: 25px 0px; ; }\n","    table tr:first-child { background-color: #FFF}\n","    table thead th { background-color: #eee; color: #000; text-align: center;}\n","    tr, th, td { border: 1px solid #ccc; border-width: 1px 0 0 1px; border-collapse: collapse;\n","    padding: 5px; }\n","    tr:nth-child(even) {background: #eee;}</style>"]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WP-LjneLzeNq"},"outputs":[],"source":["get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n","\n","dls_lm = DataBlock(\n","    blocks=TextBlock.from_folder(path, is_lm=True),\n","    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",").dataloaders(path, path=path, bs=128, seq_len=80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FcuP5dxPzeNq"},"outputs":[],"source":["dls_lm.show_batch(max_n=2)"]},{"cell_type":"markdown","metadata":{"id":"9sC5Z3XhzeNq"},"source":["### Fine-Tuning the Language Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAHdpBREzeNq"},"outputs":[],"source":["learn = language_model_learner(\n","    dls_lm, AWD_LSTM, drop_mult=0.3, \n","    metrics=[accuracy, Perplexity()]).to_fp16()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m40N88zszeNq"},"outputs":[],"source":["learn.fit_one_cycle(1, 2e-2)"]},{"cell_type":"markdown","metadata":{"id":"roBCy6CgzeNq"},"source":["### Saving and Loading Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85vAfJXRzeNq"},"outputs":[],"source":["learn.save('1epoch')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DGrLwfizeNq"},"outputs":[],"source":["learn = learn.load('1epoch')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j56f3iJkzeNq"},"outputs":[],"source":["learn.unfreeze()\n","learn.fit_one_cycle(10, 2e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMjCudkUzeNq"},"outputs":[],"source":["learn.save_encoder('finetuned')"]},{"cell_type":"markdown","metadata":{"id":"13dt6FubzeNr"},"source":["### Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zmf6r3IVzeNr"},"outputs":[],"source":["TEXT = \"I liked this movie because\"\n","N_WORDS = 40\n","N_SENTENCES = 2\n","preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n","         for _ in range(N_SENTENCES)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfAILiQzzeNr"},"outputs":[],"source":["print(\"\\n\".join(preds))"]},{"cell_type":"markdown","metadata":{"id":"UrjBBdN8zeNr"},"source":["### Creating the Classifier DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUwUH0dIzeNr"},"outputs":[],"source":["dls_clas = DataBlock(\n","    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n","    get_y = parent_label,\n","    get_items=partial(get_text_files, folders=['train', 'test']),\n","    splitter=GrandparentSplitter(valid_name='test')\n",").dataloaders(path, path=path, bs=128, seq_len=72)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5BhscGkzeNr"},"outputs":[],"source":["dls_clas.show_batch(max_n=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Oq_JJRCzeNr"},"outputs":[],"source":["nums_samp = toks200[:10].map(num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGeRPogSzeNr"},"outputs":[],"source":["nums_samp.map(len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CybAfWSSzeNr"},"outputs":[],"source":["learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n","                                metrics=accuracy).to_fp16()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZU9JeFzlzeNr"},"outputs":[],"source":["learn = learn.load_encoder('finetuned')"]},{"cell_type":"markdown","metadata":{"id":"ZYO9gvObzeNs"},"source":["### Fine-Tuning the Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WdRrpw8zeNs"},"outputs":[],"source":["learn.fit_one_cycle(1, 2e-2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fw0tZIGrzeNs"},"outputs":[],"source":["learn.freeze_to(-2)\n","learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNShDH9wzeNs"},"outputs":[],"source":["learn.freeze_to(-3)\n","learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_qNxK4azeNs"},"outputs":[],"source":["learn.unfreeze()\n","learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"]},{"cell_type":"markdown","metadata":{"id":"qLw5dhw3zeNs"},"source":["## Disinformation and Language Models"]},{"cell_type":"markdown","metadata":{"id":"5NHOW8_uzeNs"},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{"id":"vsotx6gnzeNs"},"source":["## Questionnaire"]},{"cell_type":"markdown","metadata":{"id":"iploxy-4zeNs"},"source":["1. What is \"self-supervised learning\"?\n","1. What is a \"language model\"?\n","1. Why is a language model considered self-supervised?\n","1. What are self-supervised models usually used for?\n","1. Why do we fine-tune language models?\n","1. What are the three steps to create a state-of-the-art text classifier?\n","1. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n","1. What are the three steps to prepare your data for a language model?\n","1. What is \"tokenization\"? Why do we need it?\n","1. Name three different approaches to tokenization.\n","1. What is `xxbos`?\n","1. List four rules that fastai applies to text during tokenization.\n","1. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n","1. What is \"numericalization\"?\n","1. Why might there be words that are replaced with the \"unknown word\" token?\n","1. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book's website.)\n","1. Why do we need padding for text classification? Why don't we need it for language modeling?\n","1. What does an embedding matrix for NLP contain? What is its shape?\n","1. What is \"perplexity\"?\n","1. Why do we have to pass the vocabulary of the language model to the classifier data block?\n","1. What is \"gradual unfreezing\"?\n","1. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?"]},{"cell_type":"markdown","metadata":{"id":"NML_9X9BzeNt"},"source":["### Further Research"]},{"cell_type":"markdown","metadata":{"id":"Z6NDQCNpzeNt"},"source":["1. See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\n","1. Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"df8aVMkJzeNt"},"outputs":[],"source":[""]}],"metadata":{"jupytext":{"split_at_heading":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"colab":{"name":"10_nlp.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}